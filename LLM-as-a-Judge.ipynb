{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexmof/LLM-as-a-Judge/blob/main/LLM-as-a-Judge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GPT download"
      ],
      "metadata": {
        "id": "WMLayKFuH9sS"
      },
      "id": "WMLayKFuH9sS"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def download_and_load_gpt2(model_size, models_dir):\n",
        "    # Validate model size\n",
        "    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n",
        "    if model_size not in allowed_sizes:\n",
        "        raise ValueError(f\"Model size not in {allowed_sizes}\")\n",
        "\n",
        "    # Define paths\n",
        "    model_dir = os.path.join(models_dir, model_size)\n",
        "    base_url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
        "    backup_base_url = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2\"\n",
        "    filenames = [\n",
        "        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n",
        "        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n",
        "        \"model.ckpt.meta\", \"vocab.bpe\"\n",
        "    ]\n",
        "\n",
        "    # Download files\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    for filename in filenames:\n",
        "        file_url = os.path.join(base_url, model_size, filename)\n",
        "        backup_url = os.path.join(backup_base_url, model_size, filename)\n",
        "        file_path = os.path.join(model_dir, filename)\n",
        "        download_file(file_url, file_path, backup_url)\n",
        "\n",
        "    # Load settings and params\n",
        "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
        "    settings = json.load(open(os.path.join(model_dir, \"hparams.json\"), \"r\", encoding=\"utf-8\"))\n",
        "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n",
        "\n",
        "    return settings, params\n",
        "\n",
        "\n",
        "def download_file(url, destination, backup_url=None):\n",
        "    def _attempt_download(download_url):\n",
        "        response = requests.get(download_url, stream=True, timeout=60)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        file_size = int(response.headers.get(\"Content-Length\", 0))\n",
        "\n",
        "        # Check if file exists and has same size\n",
        "        if os.path.exists(destination):\n",
        "            file_size_local = os.path.getsize(destination)\n",
        "            if file_size and file_size == file_size_local:\n",
        "                print(f\"File already exists and is up-to-date: {destination}\")\n",
        "                return True\n",
        "\n",
        "        block_size = 1024  # 1 KB\n",
        "        desc = os.path.basename(download_url)\n",
        "        with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=desc) as progress_bar:\n",
        "            with open(destination, \"wb\") as file:\n",
        "                for chunk in response.iter_content(chunk_size=block_size):\n",
        "                    if chunk:\n",
        "                        file.write(chunk)\n",
        "                        progress_bar.update(len(chunk))\n",
        "        return True\n",
        "\n",
        "    try:\n",
        "        if _attempt_download(url):\n",
        "            return\n",
        "    except requests.exceptions.RequestException:\n",
        "        if backup_url is not None:\n",
        "            print(f\"Primary URL ({url}) failed. Attempting backup URL: {backup_url}\")\n",
        "            try:\n",
        "                if _attempt_download(backup_url):\n",
        "                    return\n",
        "            except requests.exceptions.RequestException:\n",
        "                pass\n",
        "\n",
        "        error_message = (\n",
        "            f\"Failed to download from both primary URL ({url})\"\n",
        "            f\"{' and backup URL (' + backup_url + ')' if backup_url else ''}.\"\n",
        "            \"\\nCheck your internet connection or the file availability.\\n\"\n",
        "            \"For help, visit: https://github.com/rasbt/LLMs-from-scratch/discussions/273\"\n",
        "        )\n",
        "        print(error_message)\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "\n",
        "def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n",
        "    # Initialize parameters dictionary with empty blocks for each layer\n",
        "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
        "\n",
        "    # Iterate over each variable in the checkpoint\n",
        "    for name, _ in tf.train.list_variables(ckpt_path):\n",
        "        # Load the variable and remove singleton dimensions\n",
        "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
        "\n",
        "        # Process the variable name to extract relevant parts\n",
        "        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
        "\n",
        "        # Identify the target dictionary for the variable\n",
        "        target_dict = params\n",
        "        if variable_name_parts[0].startswith(\"h\"):\n",
        "            layer_number = int(variable_name_parts[0][1:])\n",
        "            target_dict = params[\"blocks\"][layer_number]\n",
        "\n",
        "        # Recursively access or create nested dictionaries\n",
        "        for key in variable_name_parts[1:-1]:\n",
        "            target_dict = target_dict.setdefault(key, {})\n",
        "\n",
        "        # Assign the variable array to the last key\n",
        "        last_key = variable_name_parts[-1]\n",
        "        target_dict[last_key] = variable_array\n",
        "\n",
        "    return params"
      ],
      "metadata": {
        "id": "69utYiM7H7uU"
      },
      "id": "69utYiM7H7uU",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Utils"
      ],
      "metadata": {
        "id": "qLSdBDRPIwoE"
      },
      "id": "qLSdBDRPIwoE"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "#####################################\n",
        "# Chapter 2\n",
        "#####################################\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "#####################################\n",
        "# Chapter 3\n",
        "#####################################\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec)  # optional projection\n",
        "\n",
        "        return context_vec\n",
        "\n",
        "\n",
        "#####################################\n",
        "# Chapter 4\n",
        "#####################################\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_resid(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed-forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_resid(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # Crop current context if it exceeds the supported context size\n",
        "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
        "        # then only the last 5 tokens are used as context\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Get the idx of the vocab entry with the highest logits value\n",
        "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "#####################################\n",
        "# Chapter 5\n",
        "#####################################\n",
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # New (not in book): numerical stability tip to get equivalent results on mps device\n",
        "            # subtract rowwise max before softmax\n",
        "            logits = logits - logits.max(dim=-1, keepdim=True).values\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward()  # Calculate loss gradients\n",
        "            optimizer.step()  # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel()\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        generate_and_print_sample(\n",
        "            model, tokenizer, device, start_context\n",
        "        )\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen\n",
        "\n",
        "\n",
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss\n",
        "\n",
        "\n",
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()\n",
        "\n",
        "\n",
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))\n",
        "\n",
        "\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params[\"wpe\"])\n",
        "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params[\"wte\"])\n",
        "\n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        q_w, k_w, v_w = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
        "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
        "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
        "\n",
        "        q_b, k_b, v_b = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
        "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
        "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
        "\n",
        "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.weight,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.bias,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].norm1.scale = assign(\n",
        "            gpt.trf_blocks[b].norm1.scale,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm1.shift = assign(\n",
        "            gpt.trf_blocks[b].norm1.shift,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "        gpt.trf_blocks[b].norm2.scale = assign(\n",
        "            gpt.trf_blocks[b].norm2.scale,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm2.shift = assign(\n",
        "            gpt.trf_blocks[b].norm2.shift,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
        "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
        "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
        "\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0)  # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "\n",
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches\n",
        "\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
        "\n",
        "    # Create a second x-axis for tokens seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(\"loss-plot.pdf\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "PsLbx8FuNjHC"
      },
      "id": "PsLbx8FuNjHC",
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c2520ec3-722f-4f44-bdd1-885b13e7afbf",
      "metadata": {
        "id": "c2520ec3-722f-4f44-bdd1-885b13e7afbf"
      },
      "source": [
        "# Finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "capítulo 07 como código base"
      ],
      "metadata": {
        "id": "jSYuOKYZFBDE"
      },
      "id": "jSYuOKYZFBDE"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "4e19327b-6c02-4881-ad02-9b6d3ec0b1b4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e19327b-6c02-4881-ad02-9b6d3ec0b1b4",
        "outputId": "4d278155-c90b-459d-bb9d-e5e151f0935e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numpy version: 2.0.2\n",
            "matplotlib version: 3.10.0\n",
            "tiktoken version: 0.12.0\n",
            "torch version: 2.10.0+cu128\n",
            "tqdm version: 4.67.3\n",
            "tensorflow version: 2.19.0\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "pkgs = [\n",
        "    \"numpy\",\n",
        "    \"matplotlib\",\n",
        "    \"tiktoken\",\n",
        "    \"torch\",\n",
        "    \"tqdm\",\n",
        "    \"tensorflow\",\n",
        "]\n",
        "for p in pkgs:\n",
        "    print(f\"{p} version: {version(p)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "0G3axLw6kY1N",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0G3axLw6kY1N",
        "outputId": "e20bcb48-957e-4e72-f490-fb5cfec35180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of entries: 1100\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import requests\n",
        "\n",
        "\n",
        "def download_and_load_file(file_path, url):\n",
        "    if not os.path.exists(file_path):\n",
        "        response = requests.get(url, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        text_data = response.text\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(text_data)\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "file_path = \"instruction-data.json\"\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
        "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
        ")\n",
        "\n",
        "data = download_and_load_file(file_path, url)\n",
        "print(\"Number of entries:\", len(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "-LiuBMsHkzQV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LiuBMsHkzQV",
        "outputId": "cd09152a-a8d7-452c-bfd7-083e2653c1d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example entry:\n",
            " {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n"
          ]
        }
      ],
      "source": [
        "print(\"Example entry:\\n\", data[50])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "uFInFxDDk2Je",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFInFxDDk2Je",
        "outputId": "e862638f-791a-405c-d5a7-ddfe5b18de26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Another example entry:\n",
            " {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"}\n"
          ]
        }
      ],
      "source": [
        "print(\"Another example entry:\\n\", data[999])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "Jhk37nnJnkBh",
      "metadata": {
        "id": "Jhk37nnJnkBh"
      },
      "outputs": [],
      "source": [
        "def format_input(entry):\n",
        "    instruction_text = (\n",
        "        f\"Below is an instruction that describes a task. \"\n",
        "        f\"Write a response that appropriately completes the request.\"\n",
        "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
        "    )\n",
        "\n",
        "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
        "\n",
        "    return instruction_text + input_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "F9UQRfjzo4Js",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9UQRfjzo4Js",
        "outputId": "e6362156-0da1-4d84-bf4f-7f9b64b1efc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Identify the correct spelling of the following word.\n",
            "\n",
            "### Input:\n",
            "Ocassion\n",
            "\n",
            "### Response:\n",
            "The correct spelling is 'Occasion.'\n"
          ]
        }
      ],
      "source": [
        "model_input = format_input(data[50])\n",
        "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
        "\n",
        "print(model_input + desired_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "a3891fa9-f738-41cd-946c-80ef9a99c346",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3891fa9-f738-41cd-946c-80ef9a99c346",
        "outputId": "ff3da69f-adca-4c3e-bd46-ad6da75f8333"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is an antonym of 'complicated'?\n",
            "\n",
            "### Response:\n",
            "An antonym of 'complicated' is 'simple'.\n"
          ]
        }
      ],
      "source": [
        "model_input = format_input(data[999])\n",
        "desired_response = f\"\\n\\n### Response:\\n{data[999]['output']}\"\n",
        "\n",
        "print(model_input + desired_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "aFZVopbIlNfx",
      "metadata": {
        "id": "aFZVopbIlNfx"
      },
      "outputs": [],
      "source": [
        "train_portion = int(len(data) * 0.85)  # 85% for training\n",
        "test_portion = int(len(data) * 0.1)    # 10% for testing\n",
        "val_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation\n",
        "\n",
        "train_data = data[:train_portion]\n",
        "test_data = data[train_portion:train_portion + test_portion]\n",
        "val_data = data[train_portion + test_portion:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "-zf6oht6bIUQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zf6oht6bIUQ",
        "outputId": "b1c3f8bd-87e3-44c9-d740-287ed2dac2e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set length: 935\n",
            "Validation set length: 55\n",
            "Test set length: 110\n"
          ]
        }
      ],
      "source": [
        "print(\"Training set length:\", len(train_data))\n",
        "print(\"Validation set length:\", len(val_data))\n",
        "print(\"Test set length:\", len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "adc29dc4-f1c7-4c71-937b-95119d6239bb",
      "metadata": {
        "id": "adc29dc4-f1c7-4c71-937b-95119d6239bb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class InstructionDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "\n",
        "        # Pre-tokenize texts\n",
        "        self.encoded_texts = []\n",
        "        for entry in data:\n",
        "            instruction_plus_input = format_input(entry)\n",
        "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
        "            full_text = instruction_plus_input + response_text\n",
        "            self.encoded_texts.append(\n",
        "                tokenizer.encode(full_text)\n",
        "            )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.encoded_texts[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "ff24fe1a-5746-461c-ad3d-b6d84a1a7c96",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff24fe1a-5746-461c-ad3d-b6d84a1a7c96",
        "outputId": "ad4df811-8a51-494c-f6e0-c0ed721f669a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50256]\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "eb4c77dd-c956-4a1b-897b-b466909f18ca",
      "metadata": {
        "id": "eb4c77dd-c956-4a1b-897b-b466909f18ca"
      },
      "outputs": [],
      "source": [
        "def custom_collate_draft_1(\n",
        "    batch,\n",
        "    pad_token_id=50256,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    # Find the longest sequence in the batch\n",
        "    # and increase the max length by +1, which will add one extra\n",
        "    # padding token below\n",
        "    batch_max_length = max(len(item)+1 for item in batch)\n",
        "\n",
        "    # Pad and prepare inputs\n",
        "    inputs_lst = []\n",
        "\n",
        "    for item in batch:\n",
        "        new_item = item.copy()\n",
        "        # Add an <|endoftext|> token\n",
        "        new_item += [pad_token_id]\n",
        "        # Pad sequences to batch_max_length\n",
        "        padded = (\n",
        "            new_item + [pad_token_id] *\n",
        "            (batch_max_length - len(new_item))\n",
        "        )\n",
        "        # Via padded[:-1], we remove the extra padded token\n",
        "        # that has been added via the +1 setting in batch_max_length\n",
        "        # (the extra padding token will be relevant in later codes)\n",
        "        inputs = torch.tensor(padded[:-1])\n",
        "        inputs_lst.append(inputs)\n",
        "\n",
        "    # Convert list of inputs to tensor and transfer to target device\n",
        "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "    return inputs_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "8fb02373-59b3-4f3a-b1d1-8181a2432645",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fb02373-59b3-4f3a-b1d1-8181a2432645",
        "outputId": "393edfc9-d67a-4791-a43b-487ab9609bb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     1,     2,     3,     4],\n",
            "        [    5,     6, 50256, 50256, 50256],\n",
            "        [    7,     8,     9, 50256, 50256]])\n"
          ]
        }
      ],
      "source": [
        "inputs_1 = [0, 1, 2, 3, 4]\n",
        "inputs_2 = [5, 6]\n",
        "inputs_3 = [7, 8, 9]\n",
        "\n",
        "batch = (\n",
        "    inputs_1,\n",
        "    inputs_2,\n",
        "    inputs_3\n",
        ")\n",
        "\n",
        "print(custom_collate_draft_1(batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "74af192e-757c-4c0a-bdf9-b7eb25bf6ebc",
      "metadata": {
        "id": "74af192e-757c-4c0a-bdf9-b7eb25bf6ebc"
      },
      "outputs": [],
      "source": [
        "def custom_collate_draft_2(\n",
        "    batch,\n",
        "    pad_token_id=50256,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    # Find the longest sequence in the batch\n",
        "    batch_max_length = max(len(item)+1 for item in batch)\n",
        "\n",
        "    # Pad and prepare inputs\n",
        "    inputs_lst, targets_lst = [], []\n",
        "\n",
        "    for item in batch:\n",
        "        new_item = item.copy()\n",
        "        # Add an <|endoftext|> token\n",
        "        new_item += [pad_token_id]\n",
        "        # Pad sequences to max_length\n",
        "        padded = (\n",
        "            new_item + [pad_token_id] *\n",
        "            (batch_max_length - len(new_item))\n",
        "        )\n",
        "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
        "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
        "        inputs_lst.append(inputs)\n",
        "        targets_lst.append(targets)\n",
        "\n",
        "    # Convert list of inputs to tensor and transfer to target device\n",
        "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "    targets_tensor = torch.stack(targets_lst).to(device)\n",
        "    return inputs_tensor, targets_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "6eb2bce3-28a7-4f39-9d4b-5e972d69066c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eb2bce3-28a7-4f39-9d4b-5e972d69066c",
        "outputId": "3ad5c547-4b17-4cf4-900e-3c9cda3ef769"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     1,     2,     3,     4],\n",
            "        [    5,     6, 50256, 50256, 50256],\n",
            "        [    7,     8,     9, 50256, 50256]])\n",
            "tensor([[    1,     2,     3,     4, 50256],\n",
            "        [    6, 50256, 50256, 50256, 50256],\n",
            "        [    8,     9, 50256, 50256, 50256]])\n"
          ]
        }
      ],
      "source": [
        "inputs, targets = custom_collate_draft_2(batch)\n",
        "print(inputs)\n",
        "print(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "41ec6e2d-9eb2-4124-913e-d2af39be4cf2",
      "metadata": {
        "id": "41ec6e2d-9eb2-4124-913e-d2af39be4cf2"
      },
      "outputs": [],
      "source": [
        "def custom_collate_fn(\n",
        "    batch,\n",
        "    pad_token_id=50256,\n",
        "    ignore_index=-100,\n",
        "    allowed_max_length=None,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    # Find the longest sequence in the batch\n",
        "    batch_max_length = max(len(item)+1 for item in batch)\n",
        "\n",
        "    # Pad and prepare inputs and targets\n",
        "    inputs_lst, targets_lst = [], []\n",
        "\n",
        "    for item in batch:\n",
        "        new_item = item.copy()\n",
        "        # Add an <|endoftext|> token\n",
        "        new_item += [pad_token_id]\n",
        "        # Pad sequences to max_length\n",
        "        padded = (\n",
        "            new_item + [pad_token_id] *\n",
        "            (batch_max_length - len(new_item))\n",
        "        )\n",
        "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
        "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
        "\n",
        "        # New: Replace all but the first padding tokens in targets by ignore_index\n",
        "        mask = targets == pad_token_id\n",
        "        indices = torch.nonzero(mask).squeeze()\n",
        "        if indices.numel() > 1:\n",
        "            targets[indices[1:]] = ignore_index\n",
        "\n",
        "        # New: Optionally truncate to maximum sequence length\n",
        "        if allowed_max_length is not None:\n",
        "            inputs = inputs[:allowed_max_length]\n",
        "            targets = targets[:allowed_max_length]\n",
        "\n",
        "        inputs_lst.append(inputs)\n",
        "        targets_lst.append(targets)\n",
        "\n",
        "    # Convert list of inputs and targets to tensors and transfer to target device\n",
        "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "    targets_tensor = torch.stack(targets_lst).to(device)\n",
        "\n",
        "    return inputs_tensor, targets_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "cdf5eec4-9ebe-4be0-9fca-9a47bee88fdc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdf5eec4-9ebe-4be0-9fca-9a47bee88fdc",
        "outputId": "abe3d05c-d33e-4918-ea0c-997032b9d1d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     1,     2,     3,     4],\n",
            "        [    5,     6, 50256, 50256, 50256],\n",
            "        [    7,     8,     9, 50256, 50256]])\n",
            "tensor([[    1,     2,     3,     4, 50256],\n",
            "        [    6, 50256,  -100,  -100,  -100],\n",
            "        [    8,     9, 50256,  -100,  -100]])\n"
          ]
        }
      ],
      "source": [
        "inputs, targets = custom_collate_fn(batch)\n",
        "print(inputs)\n",
        "print(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "W2jvh-OP9MFV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2jvh-OP9MFV",
        "outputId": "0fea6be2-5406-46fd-c11c-5b4d6c6e8680"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.1269)\n"
          ]
        }
      ],
      "source": [
        "logits_1 = torch.tensor(\n",
        "    [[-1.0, 1.0],  # 1st training example\n",
        "     [-0.5, 1.5]]  # 2nd training example\n",
        ")\n",
        "targets_1 = torch.tensor([0, 1])\n",
        "\n",
        "\n",
        "loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\n",
        "print(loss_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "nvVMuil89v9N",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvVMuil89v9N",
        "outputId": "3bfbfc43-5091-47da-e78d-f9e274da108f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.7936)\n"
          ]
        }
      ],
      "source": [
        "logits_2 = torch.tensor(\n",
        "    [[-1.0, 1.0],\n",
        "     [-0.5, 1.5],\n",
        "     [-0.5, 1.5]]  # New 3rd training example\n",
        ")\n",
        "targets_2 = torch.tensor([0, 1, 1])\n",
        "\n",
        "loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\n",
        "print(loss_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "RTyB1vah9p56",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTyB1vah9p56",
        "outputId": "b46a6e6a-d771-41b1-a037-d6d2a860fa89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.1269)\n",
            "loss_1 == loss_3: tensor(True)\n"
          ]
        }
      ],
      "source": [
        "targets_3 = torch.tensor([0, 1, -100])\n",
        "\n",
        "loss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)\n",
        "print(loss_3)\n",
        "print(\"loss_1 == loss_3:\", loss_1 == loss_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "etpqqWh8phKc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etpqqWh8phKc",
        "outputId": "6e78ac0a-ff4e-46b0-d1f4-08c7d64a231a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    # Use PyTorch 2.9 or newer for stable mps results\n",
        "    major, minor = map(int, torch.__version__.split(\".\")[:2])\n",
        "    if (major, minor) >= (2, 9):\n",
        "        device = torch.device(\"mps\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "4e47fb30-c2c6-4e6d-a64c-76cc65be4a2c",
      "metadata": {
        "id": "4e47fb30-c2c6-4e6d-a64c-76cc65be4a2c"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "customized_collate_fn = partial(\n",
        "    custom_collate_fn,\n",
        "    device=device,\n",
        "    allowed_max_length=1024\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "BtWkgir6Hlpe",
      "metadata": {
        "id": "BtWkgir6Hlpe"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 8\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_dataset = InstructionDataset(train_data, tokenizer)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=num_workers\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "1d097dc8-ad34-4f05-b435-e4147965f532",
      "metadata": {
        "id": "1d097dc8-ad34-4f05-b435-e4147965f532"
      },
      "outputs": [],
      "source": [
        "val_dataset = InstructionDataset(val_data, tokenizer)\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "test_dataset = InstructionDataset(test_data, tokenizer)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "GGs1AI3vHpnX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGs1AI3vHpnX",
        "outputId": "708a88af-669a-4e87-de7b-10f0339859cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader:\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 79]) torch.Size([8, 79])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 59]) torch.Size([8, 59])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 89]) torch.Size([8, 89])\n",
            "torch.Size([8, 59]) torch.Size([8, 59])\n",
            "torch.Size([8, 88]) torch.Size([8, 88])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 58]) torch.Size([8, 58])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 87]) torch.Size([8, 87])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 57]) torch.Size([8, 57])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 81]) torch.Size([8, 81])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 82]) torch.Size([8, 82])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 78]) torch.Size([8, 78])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n"
          ]
        }
      ],
      "source": [
        "print(\"Train loader:\")\n",
        "for inputs, targets in train_loader:\n",
        "    print(inputs.shape, targets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "21b8fd02-014f-4481-9b71-5bfee8f9dfcd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21b8fd02-014f-4481-9b71-5bfee8f9dfcd",
        "outputId": "cbc3045c-4783-46a3-bec6-a10fc9117459"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
            "          257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n",
            "        21017, 46486,    25,   198, 30003,  6525,   262,  6827,  1262,   257,\n",
            "          985,   576,    13,   198,   198, 21017, 23412,    25,   198,   464,\n",
            "         5156,   318,   845, 13779,    13,   198,   198, 21017, 18261,    25,\n",
            "          198,   464,  5156,   318,   355, 13779,   355,   257,  4936,    13,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "print(inputs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "51649ab4-1a7e-4a9e-92c5-950a24fde211",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51649ab4-1a7e-4a9e-92c5-950a24fde211",
        "outputId": "c94513cd-3ed4-4c31-a8b2-4c372d73f565"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([  318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,   257,\n",
            "         2882,   326, 20431, 32543,   262,  2581,    13,   198,   198, 21017,\n",
            "        46486,    25,   198, 30003,  6525,   262,  6827,  1262,   257,   985,\n",
            "          576,    13,   198,   198, 21017, 23412,    25,   198,   464,  5156,\n",
            "          318,   845, 13779,    13,   198,   198, 21017, 18261,    25,   198,\n",
            "          464,  5156,   318,   355, 13779,   355,   257,  4936,    13, 50256,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "print(targets[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "0d249d67-5eba-414e-9bd2-972ebf01329d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d249d67-5eba-414e-9bd2-972ebf01329d",
        "outputId": "e7653926-5db6-4d41-c86f-22a7a2a67def"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/355M/checkpoint\n",
            "File already exists and is up-to-date: gpt2/355M/encoder.json\n",
            "File already exists and is up-to-date: gpt2/355M/hparams.json\n",
            "File already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\n",
            "File already exists and is up-to-date: gpt2/355M/model.ckpt.index\n",
            "File already exists and is up-to-date: gpt2/355M/model.ckpt.meta\n",
            "File already exists and is up-to-date: gpt2/355M/vocab.bpe\n"
          ]
        }
      ],
      "source": [
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 1024,\n",
        "    \"drop_rate\": 0.0,\n",
        "    \"qkv_bias\": True\n",
        "}\n",
        "\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
        "\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
        "\n",
        "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
        "settings, params = download_and_load_gpt2(\n",
        "    model_size=model_size,\n",
        "    models_dir=\"gpt2\"\n",
        ")\n",
        "\n",
        "model = GPTModel(BASE_CONFIG)\n",
        "load_weights_into_gpt(model, params)\n",
        "model.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "7bd32b7c-5b44-4d25-a09f-46836802ca74",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bd32b7c-5b44-4d25-a09f-46836802ca74",
        "outputId": "2d4867bd-5a95-49fb-946b-1cc9eda3ae5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "input_text = format_input(val_data[0])\n",
        "print(input_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "2e3e68e0-2627-4c65-b4e7-1e0667e4f6fa",
      "metadata": {
        "id": "2e3e68e0-2627-4c65-b4e7-1e0667e4f6fa"
      },
      "outputs": [],
      "source": [
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(input_text, tokenizer),\n",
        "    max_new_tokens=35,\n",
        "    context_size=BASE_CONFIG[\"context_length\"],\n",
        "    eos_id=50256,\n",
        ")\n",
        "generated_text = token_ids_to_text(token_ids, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "ba4a55bf-a245-48d8-beda-2838a58fb5ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba4a55bf-a245-48d8-beda-2838a58fb5ba",
        "outputId": "1aa72d2d-d0e9-4a25-fe7a-b765edcc34aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The chef cooks the meal every day.\n",
            "\n",
            "### Instruction:\n",
            "\n",
            "Convert the active sentence to passive: 'The chef cooks the\n"
          ]
        }
      ],
      "source": [
        "response_text = (\n",
        "    generated_text[len(input_text):]\n",
        "    .replace(\"### Response:\", \"\")\n",
        "    .strip()\n",
        ")\n",
        "print(response_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "d99fc6f8-63b2-43da-adbb-a7b6b92c8dd5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d99fc6f8-63b2-43da-adbb-a7b6b92c8dd5",
        "outputId": "f81d73dc-cb8d-489b-ae8d-823883035629"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 3.82590913772583\n",
            "Validation loss: 3.7619335651397705\n"
          ]
        }
      ],
      "source": [
        "model.to(device)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "with torch.no_grad():\n",
        "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "78bcf83a-1fff-4540-97c1-765c4016d5e3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78bcf83a-1fff-4540-97c1-765c4016d5e3",
        "outputId": "ad63e9f2-afc6-49f0-ca91-b6cca5711b69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 2.637, Val loss 2.626\n",
            "Ep 1 (Step 000005): Train loss 1.174, Val loss 1.103\n",
            "Ep 1 (Step 000010): Train loss 0.872, Val loss 0.944\n",
            "Ep 1 (Step 000015): Train loss 0.857, Val loss 0.906\n",
            "Ep 1 (Step 000020): Train loss 0.776, Val loss 0.881\n",
            "Ep 1 (Step 000025): Train loss 0.754, Val loss 0.859\n",
            "Ep 1 (Step 000030): Train loss 0.799, Val loss 0.836\n",
            "Ep 1 (Step 000035): Train loss 0.714, Val loss 0.808\n",
            "Ep 1 (Step 000040): Train loss 0.672, Val loss 0.806\n",
            "Ep 1 (Step 000045): Train loss 0.633, Val loss 0.789\n",
            "Ep 1 (Step 000050): Train loss 0.663, Val loss 0.783\n",
            "Ep 1 (Step 000055): Train loss 0.760, Val loss 0.763\n",
            "Ep 1 (Step 000060): Train loss 0.719, Val loss 0.743\n",
            "Ep 1 (Step 000065): Train loss 0.653, Val loss 0.735\n",
            "Ep 1 (Step 000070): Train loss 0.533, Val loss 0.729\n",
            "Ep 1 (Step 000075): Train loss 0.568, Val loss 0.729\n",
            "Ep 1 (Step 000080): Train loss 0.604, Val loss 0.725\n",
            "Ep 1 (Step 000085): Train loss 0.509, Val loss 0.710\n",
            "Ep 1 (Step 000090): Train loss 0.563, Val loss 0.691\n",
            "Ep 1 (Step 000095): Train loss 0.502, Val loss 0.681\n",
            "Ep 1 (Step 000100): Train loss 0.504, Val loss 0.677\n",
            "Ep 1 (Step 000105): Train loss 0.565, Val loss 0.670\n",
            "Ep 1 (Step 000110): Train loss 0.554, Val loss 0.666\n",
            "Ep 1 (Step 000115): Train loss 0.508, Val loss 0.663\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is prepared every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive:\n",
            "Ep 2 (Step 000120): Train loss 0.435, Val loss 0.671\n",
            "Ep 2 (Step 000125): Train loss 0.451, Val loss 0.687\n",
            "Ep 2 (Step 000130): Train loss 0.447, Val loss 0.682\n",
            "Ep 2 (Step 000135): Train loss 0.405, Val loss 0.682\n",
            "Ep 2 (Step 000140): Train loss 0.410, Val loss 0.681\n",
            "Ep 2 (Step 000145): Train loss 0.369, Val loss 0.681\n",
            "Ep 2 (Step 000150): Train loss 0.382, Val loss 0.675\n",
            "Ep 2 (Step 000155): Train loss 0.414, Val loss 0.675\n",
            "Ep 2 (Step 000160): Train loss 0.412, Val loss 0.684\n",
            "Ep 2 (Step 000165): Train loss 0.379, Val loss 0.686\n",
            "Ep 2 (Step 000170): Train loss 0.322, Val loss 0.680\n",
            "Ep 2 (Step 000175): Train loss 0.338, Val loss 0.667\n",
            "Ep 2 (Step 000180): Train loss 0.392, Val loss 0.656\n",
            "Ep 2 (Step 000185): Train loss 0.415, Val loss 0.657\n",
            "Ep 2 (Step 000190): Train loss 0.340, Val loss 0.647\n",
            "Ep 2 (Step 000195): Train loss 0.328, Val loss 0.633\n",
            "Ep 2 (Step 000200): Train loss 0.309, Val loss 0.633\n",
            "Ep 2 (Step 000205): Train loss 0.353, Val loss 0.631\n",
            "Ep 2 (Step 000210): Train loss 0.364, Val loss 0.630\n",
            "Ep 2 (Step 000215): Train loss 0.394, Val loss 0.634\n",
            "Ep 2 (Step 000220): Train loss 0.297, Val loss 0.644\n",
            "Ep 2 (Step 000225): Train loss 0.342, Val loss 0.658\n",
            "Ep 2 (Step 000230): Train loss 0.294, Val loss 0.656\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is cooked every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: What is the capital of the United Kingdom\n",
            "Training completed in 3.51 minutes.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 2\n",
        "\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "4acd368b-1403-4807-a218-9102e35bfdbb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "4acd368b-1403-4807-a218-9102e35bfdbb",
        "outputId": "c2273e2d-2998-4b87-9ed3-27b6710d44cc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "VQ2NZMbfucAc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ2NZMbfucAc",
        "outputId": "5048f8ac-1825-4788-a23e-37d09abdbac3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Rewrite the sentence using a simile.\n",
            "\n",
            "### Input:\n",
            "The car is very fast.\n",
            "\n",
            "Correct response:\n",
            ">> The car is as fast as lightning.\n",
            "\n",
            "Model response:\n",
            ">> The car is as fast as a bullet.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What type of cloud is typically associated with thunderstorms?\n",
            "\n",
            "Correct response:\n",
            ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
            "\n",
            "Model response:\n",
            ">> The type of cloud associated with thunderstorms is a cumulus cloud.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Name the author of 'Pride and Prejudice'.\n",
            "\n",
            "Correct response:\n",
            ">> Jane Austen.\n",
            "\n",
            "Model response:\n",
            ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
            "-------------------------------------\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "\n",
        "for entry in test_data[:3]:\n",
        "\n",
        "    input_text = format_input(entry)\n",
        "\n",
        "    token_ids = generate(\n",
        "        model=model,\n",
        "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "        max_new_tokens=256,\n",
        "        context_size=BASE_CONFIG[\"context_length\"],\n",
        "        eos_id=50256\n",
        "    )\n",
        "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    response_text = (\n",
        "        generated_text[len(input_text):]\n",
        "        .replace(\"### Response:\", \"\")\n",
        "        .strip()\n",
        ")\n",
        "\n",
        "    print(input_text)\n",
        "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
        "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
        "    print(\"-------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "-PNGKzY4snKP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PNGKzY4snKP",
        "outputId": "72e154bc-7b9c-46b9-acd5-db158d37d4cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 110/110 [01:07<00:00,  1.63it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
        "\n",
        "    input_text = format_input(entry)\n",
        "\n",
        "    token_ids = generate(\n",
        "        model=model,\n",
        "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "        max_new_tokens=256,\n",
        "        context_size=BASE_CONFIG[\"context_length\"],\n",
        "        eos_id=50256\n",
        "    )\n",
        "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
        "\n",
        "    test_data[i][\"model_response\"] = response_text\n",
        "\n",
        "\n",
        "with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
        "    json.dump(test_data, file, indent=4)  # \"indent\" for pretty-printing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "u-AvCCMTnPSE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-AvCCMTnPSE",
        "outputId": "d4be10fb-c79d-4ad5-dedf-2dca29d5aa16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'instruction': 'Rewrite the sentence using a simile.', 'input': 'The car is very fast.', 'output': 'The car is as fast as lightning.', 'model_response': 'The car is as fast as a bullet.'}\n"
          ]
        }
      ],
      "source": [
        "print(test_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "8cBU0iHmVfOI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cBU0iHmVfOI",
        "outputId": "8c7b3537-b155-4547-c7b7-a9d21009829f",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as gpt2-medium355M-sft.pth\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft.pth\"\n",
        "torch.save(model.state_dict(), file_name)\n",
        "print(f\"Model saved as {file_name}\")\n",
        "\n",
        "# Load model via\n",
        "# model.load_state_dict(torch.load(\"gpt2-medium355M-sft.pth\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Avaliação: LLM-as-a-Judge."
      ],
      "metadata": {
        "id": "Wu33gqqFHBgy"
      },
      "id": "Wu33gqqFHBgy"
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.colab import userdata\n",
        "\n",
        "client = genai.Client(api_key=userdata.get('GEMINI_TOKEN'))\n",
        "\n",
        "def query_model_json(prompt):\n",
        "    \"\"\"Envia o prompt para o Gemini usando o novo SDK e força a saída em JSON.\"\"\"\n",
        "    try:\n",
        "        response = client.models.generate_content(\n",
        "            model='gemini-2.5-flash',\n",
        "            contents=prompt,\n",
        "            config=types.GenerateContentConfig(\n",
        "                temperature=0.1,\n",
        "                response_mime_type=\"application/json\",\n",
        "            )\n",
        "        )\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"Erro na API: {e}\")\n",
        "        return '{\"vencedor\": \"Erro\", \"justificativa\": \"Falha na API\", \"notas_A\": {\"factual\": 0, \"aderencia\": 0, \"clareza\": 0}, \"notas_B\": {\"factual\": 0, \"aderencia\": 0, \"clareza\": 0}}'"
      ],
      "metadata": {
        "id": "fxIduWDjTOUQ"
      },
      "id": "fxIduWDjTOUQ",
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_models_ab(test_data):\n",
        "    results = []\n",
        "    for entry in test_data:\n",
        "        prompt = (\n",
        "            f\"Você é um juiz especialista em IA avaliando duas respostas de modelos de linguagem.\\n\\n\"\n",
        "            f\"Instrução: {entry['instruction']}\\n\"\n",
        "            f\"Entrada (Contexto): {entry['input']}\\n\\n\"\n",
        "            f\"Resposta A (Modelo Base): {entry['base_model_response']}\\n\"\n",
        "            f\"Resposta B (Modelo Fine-tuned): {entry['finetuned_model_response']}\\n\\n\"\n",
        "            f\"Avalie ambas as respostas usando a seguinte rubrica (notas de 0 a 5):\\n\"\n",
        "            f\"1. Correção factual (0-5)\\n\"\n",
        "            f\"2. Aderência à instrução (0-5)\\n\"\n",
        "            f\"3. Clareza e utilidade (0-5)\\n\\n\"\n",
        "            f\"Retorne APENAS um objeto JSON válido com a seguinte estrutura estrita:\\n\"\n",
        "            f\"{{\\n\"\n",
        "            f\"  \\\"vencedor\\\": \\\"A\\\", \\\"B\\\" ou \\\"Empate\\\",\\n\"\n",
        "            f\"  \\\"justificativa\\\": \\\"Sua análise crítica detalhada aqui...\\\",\\n\"\n",
        "            f\"  \\\"notas_A\\\": {{\\\"factual\\\": 0, \\\"aderencia\\\": 0, \\\"clareza\\\": 0}},\\n\"\n",
        "            f\"  \\\"notas_B\\\": {{\\\"factual\\\": 0, \\\"aderencia\\\": 0, \\\"clareza\\\": 0}}\\n\"\n",
        "            f\"}}\"\n",
        "        )\n",
        "\n",
        "        print(f\"Avaliando instrução: {entry['instruction'][:50]}...\")\n",
        "\n",
        "        json_string = query_model_json(prompt)\n",
        "\n",
        "        try:\n",
        "            evaluation_dict = json.loads(json_string)\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"Aviso: O modelo não retornou um JSON válido. Salvando como string.\")\n",
        "            evaluation_dict = {\"erro_parse\": json_string}\n",
        "\n",
        "        results.append({\n",
        "            \"instruction\": entry['instruction'],\n",
        "            \"judge_evaluation\": evaluation_dict\n",
        "        })\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "1-jvCstZVWCe"
      },
      "id": "1-jvCstZVWCe",
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dados_teste_com_respostas = [\n",
        "    {\n",
        "        \"instruction\": \"Responda à pergunta usando apenas o contexto.\",\n",
        "        \"input\": \"Pergunta: Quem fundou a Microsoft?\\nContexto: A Microsoft foi fundada por Bill Gates e Paul Allen em 1975.\",\n",
        "        \"base_model_response\": \"A Microsoft é uma empresa de software muito grande. Windows.\",\n",
        "        \"finetuned_model_response\": \"Bill Gates e Paul Allen.\"\n",
        "    },\n",
        "]\n",
        "\n",
        "print(f\"Iniciando a avaliação de {len(dados_teste_com_respostas)} exemplos com o Gemini...\")\n",
        "\n",
        "resultados_finais = evaluate_models_ab(dados_teste_com_respostas)\n",
        "\n",
        "with open(\"resultados_avaliacao_juiz.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(resultados_finais, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(\"Avaliação concluída e salva no arquivo resultados_avaliacao_juiz.json!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cy7RmWotWNkE",
        "outputId": "67a7e0a3-0ea0-493a-cfaa-25994f90cf69"
      },
      "id": "cy7RmWotWNkE",
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando a avaliação de 1 exemplos com o Gemini...\n",
            "Avaliando instrução: Responda à pergunta usando apenas o contexto....\n",
            "Avaliação concluída e salva no arquivo resultados_avaliacao_juiz.json!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Geração de respostas finetuned e base"
      ],
      "metadata": {
        "id": "jAtLJcvDX3QQ"
      },
      "id": "jAtLJcvDX3QQ"
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def format_prompt(entry):\n",
        "    return f\"Instrução: {entry['instruction']}\\nEntrada: {entry['input']}\\nResposta:\\n\"\n",
        "\n",
        "def get_model_response(model, tokenizer, prompt, max_new_tokens=100, context_size=1024):\n",
        "    model.eval()\n",
        "\n",
        "    input_ids_list = tokenizer.encode(prompt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "    idx = torch.tensor(input_ids_list).unsqueeze(0).to(device)\n",
        "\n",
        "    eos_id = 50256\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -context_size:]\n",
        "\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "            logits = logits[:, -1, :]\n",
        "\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "            if idx_next.item() == eos_id:\n",
        "                break\n",
        "\n",
        "    generated_ids_tensor = idx[0][len(input_ids_list):]\n",
        "\n",
        "    generated_ids_list = generated_ids_tensor.tolist()\n",
        "    response = tokenizer.decode(generated_ids_list)\n",
        "\n",
        "    return response.strip()"
      ],
      "metadata": {
        "id": "9i649OLnX1t3"
      },
      "id": "9i649OLnX1t3",
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ETAPA A: Inferência com o Modelo Base\n",
        "\n"
      ],
      "metadata": {
        "id": "ETl2s2IcaB1f"
      },
      "id": "ETl2s2IcaB1f"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(\"Gerando respostas com o Modelo Base (Pré-treinado)...\")\n",
        "\n",
        "resultados_inferencia = []\n",
        "\n",
        "for entry in tqdm(test_data):\n",
        "    prompt = format_prompt(entry)\n",
        "    resposta_base = get_model_response(model, tokenizer, prompt)\n",
        "\n",
        "    resultados_inferencia.append({\n",
        "        \"instruction\": entry[\"instruction\"],\n",
        "        \"input\": entry[\"input\"],\n",
        "        \"expected_output\": entry[\"output\"],\n",
        "        \"base_model_response\": resposta_base\n",
        "    })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkKvVyVFZ-N-",
        "outputId": "fcb038fd-d52c-4cb9-a24e-6246418a9f6f"
      },
      "id": "CkKvVyVFZ-N-",
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gerando respostas com o Modelo Base (Pré-treinado)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 110/110 [00:48<00:00,  2.25it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ETAPA B: Inferência com o Modelo Fine-Tuned"
      ],
      "metadata": {
        "id": "R2k0mn9yaNHx"
      },
      "id": "R2k0mn9yaNHx"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nCarregando pesos do modelo Fine-Tuned...\")\n",
        "\n",
        "model.load_state_dict(torch.load(\"/content/gpt2-medium355M-sft.pth\"))\n",
        "\n",
        "print(\"Gerando respostas com o Modelo Fine-Tuned...\")\n",
        "for i, entry in enumerate(tqdm(test_data)):\n",
        "    prompt = format_prompt(entry)\n",
        "    resposta_ft = get_model_response(model, tokenizer, prompt)\n",
        "\n",
        "    resultados_inferencia[i][\"finetuned_model_response\"] = resposta_ft\n",
        "\n",
        "with open(\"respostas_para_avaliacao.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(resultados_inferencia, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(\"\\nInferência concluída! Arquivo 'respostas_para_avaliacao.json' salvo.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BQ6GrJmaROS",
        "outputId": "411b6d09-9f6a-412b-baed-476e7a369a40"
      },
      "id": "4BQ6GrJmaROS",
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Carregando pesos do modelo Fine-Tuned...\n",
            "Gerando respostas com o Modelo Fine-Tuned...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 110/110 [00:52<00:00,  2.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Inferência concluída! Arquivo 'respostas_para_avaliacao.json' salvo.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tentativa de geração sem sobreposição de modelos"
      ],
      "metadata": {
        "id": "pSBf7y1T93G1"
      },
      "id": "pSBf7y1T93G1"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"1. Recriando a arquitetura do modelo do zero para garantir pureza...\")\n",
        "\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "# --- ETAPA A: Inferência com o Modelo Base Real ---\n",
        "print(\"2. Gerando respostas com o Modelo Base (Pré-treinado Original)...\")\n",
        "\n",
        "resultados_inferencia = []\n",
        "\n",
        "for entry in tqdm(test_data):\n",
        "    prompt = format_prompt(entry)\n",
        "    resposta_base = get_model_response(model, tokenizer, prompt)\n",
        "\n",
        "    resultados_inferencia.append({\n",
        "        \"instruction\": entry[\"instruction\"],\n",
        "        \"input\": entry[\"input\"],\n",
        "        \"expected_output\": entry[\"output\"],\n",
        "        \"base_model_response\": resposta_base\n",
        "    })\n",
        "\n",
        "# --- ETAPA B: Inferência com o Modelo Fine-Tuned ---\n",
        "print(\"\\n3. Injetando os pesos do Fine-Tuning no modelo...\")\n",
        "caminho_pesos = \"/content/gpt2-medium355M-sft.pth\"\n",
        "model.load_state_dict(torch.load(caminho_pesos, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "print(\"4. Gerando respostas com o Modelo Fine-Tuned...\")\n",
        "for i, entry in enumerate(tqdm(test_data)):\n",
        "    prompt = format_prompt(entry)\n",
        "    resposta_ft = get_model_response(model, tokenizer, prompt)\n",
        "\n",
        "    resultados_inferencia[i][\"finetuned_model_response\"] = resposta_ft\n",
        "\n",
        "with open(\"respostas_para_avaliacao.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(resultados_inferencia, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(\"\\nConcluído! O arquivo foi salvo.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pv-mXS3E5_Iz",
        "outputId": "237ecdb5-4ff7-4032-b5b3-da34b07b1d1a"
      },
      "id": "Pv-mXS3E5_Iz",
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Recriando a arquitetura do modelo do zero para garantir pureza...\n",
            "2. Gerando respostas com o Modelo Base (Pré-treinado Original)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 110/110 [00:48<00:00,  2.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "3. Injetando os pesos do Fine-Tuning no modelo...\n",
            "4. Gerando respostas com o Modelo Fine-Tuned...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 110/110 [00:47<00:00,  2.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Concluído! O arquivo blindado foi salvo.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM-as-a-Judge."
      ],
      "metadata": {
        "id": "yBRNERJFkfSi"
      },
      "id": "yBRNERJFkfSi"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lm3979Iy0Xxz",
        "outputId": "b11d4410-c02f-4f7f-b1d2-b28fd8f2da65"
      },
      "id": "Lm3979Iy0Xxz",
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-1.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n",
            "Downloading groq-1.0.0-py3-none-any.whl (138 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/138.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import time\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from groq import Groq\n",
        "\n",
        "GROQ_API_KEY = userdata.get('GROK')\n",
        "client = Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "with open(\"respostas_para_avaliacao.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    dados_inferencia = json.load(f)\n",
        "\n",
        "ARQUIVO_SAIDA = \"resultados_completos_juiz.json\"\n",
        "resultados_finais = []\n",
        "instrucoes_avaliadas = set()\n",
        "\n",
        "if os.path.exists(ARQUIVO_SAIDA):\n",
        "    with open(ARQUIVO_SAIDA, \"r\", encoding=\"utf-8\") as f:\n",
        "        try:\n",
        "            resultados_finais = json.load(f)\n",
        "            instrucoes_avaliadas = {res[\"instruction\"] for res in resultados_finais}\n",
        "            print(f\"Retomando progresso... {len(resultados_finais)} itens já avaliados encontrados.\")\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "\n",
        "def avaliar_com_juiz_groq(entry):\n",
        "    prompt = (\n",
        "        f\"Você é um juiz especialista em IA avaliando duas respostas de modelos de linguagem.\\n\\n\"\n",
        "        f\"Instrução: {entry['instruction']}\\n\"\n",
        "        f\"Entrada (Contexto): {entry['input']}\\n\\n\"\n",
        "        f\"Resposta A (Modelo Base): {entry['base_model_response']}\\n\"\n",
        "        f\"Resposta B (Modelo Fine-tuned): {entry['finetuned_model_response']}\\n\\n\"\n",
        "        f\"Avalie ambas as respostas usando a seguinte rubrica (notas de 0 a 5):\\n\"\n",
        "        f\"1. Correção factual (0-5)\\n\"\n",
        "        f\"2. Aderência à instrução (0-5)\\n\"\n",
        "        f\"3. Clareza e utilidade (0-5)\\n\\n\"\n",
        "        f\"Retorne APENAS um objeto JSON válido com a seguinte estrutura estrita:\\n\"\n",
        "        f\"{{\\n\"\n",
        "        f\"  \\\"vencedor\\\": \\\"A\\\", \\\"B\\\" ou \\\"Empate\\\",\\n\"\n",
        "        f\"  \\\"justificativa\\\": \\\"Sua análise crítica detalhada...\\\",\\n\"\n",
        "        f\"  \\\"notas_A\\\": {{\\\"factual\\\": 0, \\\"aderencia\\\": 0, \\\"clareza\\\": 0}},\\n\"\n",
        "        f\"  \\\"notas_B\\\": {{\\\"factual\\\": 0, \\\"aderencia\\\": 0, \\\"clareza\\\": 0}}\\n\"\n",
        "        f\"}}\"\n",
        "    )\n",
        "\n",
        "    for tentativa in range(3):\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "              model=\"llama-3.3-70b-versatile\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"Você é um juiz imparcial que retorna apenas JSON válido.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                temperature=0.0,\n",
        "                response_format={\"type\": \"json_object\"}\n",
        "            )\n",
        "            return json.loads(response.choices[0].message.content)\n",
        "\n",
        "        except Exception as e:\n",
        "            if \"429\" in str(e):\n",
        "                espera = 10\n",
        "                print(f\"\\n[Aviso] Limite de API atingido. Aguardando {espera}s...\")\n",
        "                time.sleep(espera)\n",
        "            else:\n",
        "                print(f\"Erro na API: {e}\")\n",
        "                return None\n",
        "    return None\n",
        "\n",
        "print(f\"Faltam avaliar {len(dados_inferencia) - len(instrucoes_avaliadas)} itens.\")\n",
        "\n",
        "for entry in tqdm(dados_inferencia):\n",
        "    if entry[\"instruction\"] in instrucoes_avaliadas:\n",
        "        continue\n",
        "\n",
        "    avaliacao = avaliar_com_juiz_groq(entry)\n",
        "\n",
        "    if avaliacao:\n",
        "        entry[\"judge_evaluation\"] = avaliacao\n",
        "        resultados_finais.append(entry)\n",
        "        instrucoes_avaliadas.add(entry[\"instruction\"])\n",
        "\n",
        "        with open(ARQUIVO_SAIDA, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(resultados_finais, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "    time.sleep(2.5)\n",
        "\n",
        "print(\"\\nAvaliação concluída com sucesso usando Groq!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "buqM3qu8kl5F",
        "outputId": "5d90d329-f388-4567-9bbc-bfffa7571f06"
      },
      "id": "buqM3qu8kl5F",
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Faltam avaliar 110 itens.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 3/110 [00:09<05:34,  3.13s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2648004906.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mavaliacao\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavaliar_com_juiz_groq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mavaliacao\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2648004906.py\u001b[0m in \u001b[0;36mavaliar_com_juiz_groq\u001b[0;34m(entry)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtentativa\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m     49\u001b[0m               \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"llama-3.3-70b-versatile\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/groq/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, citation_options, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    459\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \"\"\"\n\u001b[0;32m--> 461\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    462\u001b[0m             \u001b[0;34m\"/openai/v1/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1240\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m         )\n\u001b[0;32m-> 1242\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m    978\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m                 response = self._client.send(\n\u001b[0m\u001b[1;32m    981\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m                     \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stream_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    943\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    107\u001b[0m                 trace.return_value = (\n\u001b[1;32m    108\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1230\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1233\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1103\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}